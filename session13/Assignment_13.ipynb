{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmLLIibCa3Df",
        "colab_type": "text"
      },
      "source": [
        "Weight decay is set to 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0_OSDaU3Vha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "f3387c17-11e4-4b1e-d3e3-07e3d94f6c14"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib as tf_contrib\n",
        "\n",
        "\n",
        "# Xavier : tf_contrib.layers.xavier_initializer()\n",
        "# He : tf_contrib.layers.variance_scaling_initializer()\n",
        "# Normal : tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "# l2_decay : tf_contrib.layers.l2_regularizer(0.0001)\n",
        "\n",
        "weight_init = tf_contrib.layers.variance_scaling_initializer()\n",
        "weight_regularizer = tf_contrib.layers.l2_regularizer(5e-4)\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Layer\n",
        "##################################################################################\n",
        "\n",
        "def conv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, scope='conv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x = tf.layers.conv2d(inputs=x, filters=channels,\n",
        "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                             kernel_regularizer=weight_regularizer,\n",
        "                             strides=stride, use_bias=use_bias, padding=padding)\n",
        "\n",
        "        return x\n",
        "\n",
        "def fully_conneted(x, units, use_bias=True, scope='fully_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x = flatten(x)\n",
        "        x = tf.layers.dense(x, units=units, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='resblock') :\n",
        "    with tf.variable_scope(scope) :\n",
        "\n",
        "        x = batch_norm(x_init, is_training, scope='batch_norm_0')\n",
        "        x = relu(x)\n",
        "\n",
        "\n",
        "        if downsample :\n",
        "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
        "            x_init = conv(x_init, channels, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
        "\n",
        "        else :\n",
        "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
        "\n",
        "        x = batch_norm(x, is_training, scope='batch_norm_1')\n",
        "        x = relu(x)\n",
        "        x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_1')\n",
        "\n",
        "\n",
        "\n",
        "        return x + x_init\n",
        "\n",
        "def bottle_resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='bottle_resblock') :\n",
        "    with tf.variable_scope(scope) :\n",
        "        x = batch_norm(x_init, is_training, scope='batch_norm_1x1_front')\n",
        "        shortcut = relu(x)\n",
        "\n",
        "        x = conv(shortcut, channels, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_front')\n",
        "        x = batch_norm(x, is_training, scope='batch_norm_3x3')\n",
        "        x = relu(x)\n",
        "\n",
        "        if downsample :\n",
        "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
        "            shortcut = conv(shortcut, channels*4, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
        "\n",
        "        else :\n",
        "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
        "            shortcut = conv(shortcut, channels * 4, kernel=1, stride=1, use_bias=use_bias, scope='conv_init')\n",
        "\n",
        "        x = batch_norm(x, is_training, scope='batch_norm_1x1_back')\n",
        "        x = relu(x)\n",
        "        x = conv(x, channels*4, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_back')\n",
        "\n",
        "        return x + shortcut\n",
        "\n",
        "\n",
        "\n",
        "def get_residual_layer(res_n) :\n",
        "    x = []\n",
        "\n",
        "    if res_n == 18 :\n",
        "        x = [2, 2, 2, 2]\n",
        "\n",
        "    if res_n == 34 :\n",
        "        x = [3, 4, 6, 3]\n",
        "\n",
        "    if res_n == 50 :\n",
        "        x = [3, 4, 6, 3]\n",
        "\n",
        "    if res_n == 101 :\n",
        "        x = [3, 4, 23, 3]\n",
        "\n",
        "    if res_n == 152 :\n",
        "        x = [3, 8, 36, 3]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Sampling\n",
        "##################################################################################\n",
        "\n",
        "def flatten(x) :\n",
        "    return tf.layers.flatten(x)\n",
        "\n",
        "def global_avg_pooling(x):\n",
        "    gap = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "    return gap\n",
        "\n",
        "def avg_pooling(x) :\n",
        "    return tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='SAME')\n",
        "\n",
        "##################################################################################\n",
        "# Activation function\n",
        "##################################################################################\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Normalization function\n",
        "##################################################################################\n",
        "\n",
        "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
        "    return tf_contrib.layers.batch_norm(x,\n",
        "                                        decay=0.9, epsilon=1e-05,\n",
        "                                        center=True, scale=True, updates_collections=None,\n",
        "                                        is_training=is_training, scope=scope)\n",
        "\n",
        "##################################################################################\n",
        "# Loss function\n",
        "##################################################################################\n",
        "\n",
        "def classification_loss(logit, label) :\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit))\n",
        "    prediction = tf.equal(tf.argmax(logit, -1), tf.argmax(label, -1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "\n",
        "    return loss, accuracy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oz53ztmbA2N",
        "colab_type": "text"
      },
      "source": [
        "Use Normalization values of: (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "Random Crop of 32 with padding of 4px\n",
        "\n",
        "Horizontal Flip (0.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q13ndka3av_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3366d25-3736-45ad-9847-81d313f2d939"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import os\n",
        "from keras.datasets import cifar10, cifar100, mnist, fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy import misc\n",
        "\n",
        "def check_folder(log_dir):\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def show_all_variables():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "\n",
        "def str2bool(x):\n",
        "    return x.lower() in ('true')\n",
        "\n",
        "def load_cifar10() :\n",
        "    (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
        "    # train_data = train_data / 255.0\n",
        "    # test_data = test_data / 255.0\n",
        "\n",
        "    train_data, test_data = normalize(train_data, test_data)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 10)\n",
        "    test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "    seed = 777\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_data)\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_labels)\n",
        "\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "\n",
        "\n",
        "def normalize(X_train, X_test):\n",
        "\n",
        "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
        "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "    print(mean)\n",
        "    print(std)\n",
        "    MEAN_IMAGE = tf.constant([0.4914, 0.4822, 0.4465], dtype=tf.float32)\n",
        "    STD_IMAGE = tf.constant([0.2023, 0.1994, 0.2010], dtype=tf.float32)\n",
        "    # showing shape not \n",
        "    \n",
        "    X_train = (X_train/255 - [0.4914, 0.4822, 0.4465]) / [0.2023, 0.1994, 0.2010]\n",
        "    X_test = (X_test/255 - [0.4914, 0.4822, 0.4465]) / [0.2023, 0.1994, 0.2010]\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def get_annotations_map():\n",
        "    valAnnotationsPath = './tiny-imagenet-200/val/val_annotations.txt'\n",
        "    valAnnotationsFile = open(valAnnotationsPath, 'r')\n",
        "    valAnnotationsContents = valAnnotationsFile.read()\n",
        "    valAnnotations = {}\n",
        "\n",
        "    for line in valAnnotationsContents.splitlines():\n",
        "        pieces = line.strip().split()\n",
        "        valAnnotations[pieces[0]] = pieces[1]\n",
        "\n",
        "    return valAnnotations\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "    oshape = np.shape(batch[0])\n",
        "\n",
        "    if padding:\n",
        "        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
        "    new_batch = []\n",
        "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "    for i in range(len(batch)):\n",
        "        new_batch.append(batch[i])\n",
        "        if padding:\n",
        "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                      mode='constant', constant_values=0)\n",
        "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                       nw:nw + crop_shape[1]]\n",
        "    return new_batch\n",
        "\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "    for i in range(len(batch)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            batch[i] = np.fliplr(batch[i])\n",
        "    return batch\n",
        "\n",
        "def data_augmentation(batch, img_size, dataset_name):\n",
        "    if dataset_name == 'mnist' :\n",
        "        batch = _random_crop(batch, [img_size, img_size], 4)\n",
        "\n",
        "    elif dataset_name =='tiny' :\n",
        "        batch = _random_flip_leftright(batch)\n",
        "        batch = _random_crop(batch, [img_size, img_size], 8)\n",
        "\n",
        "    else :\n",
        "        batch = _random_flip_leftright(batch)\n",
        "        batch = _random_crop(batch, [img_size, img_size], 4)\n",
        "    return batch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWIjVcKObU2N",
        "colab_type": "text"
      },
      "source": [
        "Optimizer: SGD\n",
        "\n",
        " Use Batch Size 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F_Q8RJ8d7Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyWkudxgd8YV",
        "colab_type": "text"
      },
      "source": [
        "B1 -  64 channels\n",
        "\n",
        "B2  - 128 channels\n",
        "\n",
        "B3  - 256 channels\n",
        "\n",
        "B4  - 512 channels\n",
        "\n",
        "Are used with stride of 1 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6dEwtsfzES-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "# from ops import *\n",
        "# from utils import *\n",
        "\n",
        "class ResNet(object):\n",
        "    def __init__(self, sess, args):\n",
        "        self.model_name = 'ResNet'\n",
        "        self.sess = sess\n",
        "        # self.dataset_name = args.dataset\n",
        "        self.dataset_name = 'cifar10'\n",
        "\n",
        "        if self.dataset_name == 'cifar10' :\n",
        "            self.train_x, self.train_y, self.test_x, self.test_y = load_cifar10()\n",
        "            self.img_size = 32\n",
        "            self.c_dim = 3\n",
        "            self.label_dim = 10\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        self.checkpoint_dir =' ' #args.checkpoint_dir\n",
        "        self.log_dir = ' ' #args.log_dir\n",
        "\n",
        "        self.res_n =18 # args.res_n\n",
        "\n",
        "        self.epoch =10 #args.epoch\n",
        "        self.batch_size = 128 #args.batch_size\n",
        "        self.iteration = len(self.train_x) // self.batch_size\n",
        "\n",
        "        self.init_lr = 0.01 #args.lr\n",
        "\n",
        "\n",
        "    ##################################################################################\n",
        "    # Generator\n",
        "    ##################################################################################\n",
        "    \n",
        "    \n",
        "\n",
        "    def network(self, x, is_training=True, reuse=False):\n",
        "        \n",
        "        with tf.variable_scope(\"network\", reuse=reuse):\n",
        "\n",
        "            if self.res_n < 50 :\n",
        "                residual_block = resblock\n",
        "            else :\n",
        "                residual_block = bottle_resblock\n",
        "\n",
        "            residual_list = get_residual_layer(self.res_n)\n",
        "\n",
        "            ch = 64 # paper is 64\n",
        "\n",
        "            \n",
        "\n",
        "            x = conv(x, channels=ch, kernel=3, stride=1, scope='conv')\n",
        "\n",
        "            for i in range(residual_list[0]) :\n",
        "                x = residual_block(x, channels=ch, is_training=is_training, downsample=False, scope='resblock0_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "            x = residual_block(x, channels=ch*2, is_training=is_training, downsample=True, scope='resblock1_0')\n",
        "\n",
        "            for i in range(1, residual_list[1]) :\n",
        "                x = residual_block(x, channels=ch*2, is_training=is_training, downsample=False, scope='resblock1_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "            x = residual_block(x, channels=ch*4, is_training=is_training, downsample=True, scope='resblock2_0')\n",
        "\n",
        "            for i in range(1, residual_list[2]) :\n",
        "                x = residual_block(x, channels=ch*4, is_training=is_training, downsample=False, scope='resblock2_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "            x = residual_block(x, channels=ch*8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
        "\n",
        "            for i in range(1, residual_list[3]) :\n",
        "                x = residual_block(x, channels=ch*8, is_training=is_training, downsample=False, scope='resblock_3_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "\n",
        "            x = batch_norm(x, is_training, scope='batch_norm')\n",
        "            x = relu(x)\n",
        "\n",
        "            x = global_avg_pooling(x)\n",
        "            x = fully_conneted(x, units=self.label_dim, scope='logit')\n",
        "\n",
        "            return x\n",
        "\n",
        "    ##################################################################################\n",
        "    # Model\n",
        "    ##################################################################################\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\" Graph Input \"\"\"\n",
        "        self.train_inptus = tf.placeholder(tf.float32, [self.batch_size, self.img_size, self.img_size, self.c_dim], name='train_inputs')\n",
        "        self.train_labels = tf.placeholder(tf.float32, [self.batch_size, self.label_dim], name='train_labels')\n",
        "\n",
        "        self.test_inptus = tf.placeholder(tf.float32, [len(self.test_x), self.img_size, self.img_size, self.c_dim], name='test_inputs')\n",
        "        self.test_labels = tf.placeholder(tf.float32, [len(self.test_y), self.label_dim], name='test_labels')\n",
        "\n",
        "        self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "        \"\"\" Model \"\"\"\n",
        "        self.train_logits = self.network(self.train_inptus)\n",
        "        self.test_logits = self.network(self.test_inptus, is_training=False, reuse=True)\n",
        "\n",
        "        self.train_loss, self.train_accuracy = classification_loss(logit=self.train_logits, label=self.train_labels)\n",
        "        self.test_loss, self.test_accuracy = classification_loss(logit=self.test_logits, label=self.test_labels)\n",
        "        \n",
        "        reg_loss = tf.losses.get_regularization_loss()\n",
        "        self.train_loss += reg_loss\n",
        "        self.test_loss += reg_loss\n",
        "\n",
        "\n",
        "        \"\"\" Training \"\"\"\n",
        "        self.optim = tf.train.MomentumOptimizer(self.lr, momentum=0.9).minimize(self.train_loss)\n",
        "\n",
        "        \"\"\"\" Summary \"\"\"\n",
        "        self.summary_train_loss = tf.summary.scalar(\"train_loss\", self.train_loss)\n",
        "        self.summary_train_accuracy = tf.summary.scalar(\"train_accuracy\", self.train_accuracy)\n",
        "\n",
        "        self.summary_test_loss = tf.summary.scalar(\"test_loss\", self.test_loss)\n",
        "        self.summary_test_accuracy = tf.summary.scalar(\"test_accuracy\", self.test_accuracy)\n",
        "\n",
        "        self.train_summary = tf.summary.merge([self.summary_train_loss, self.summary_train_accuracy])\n",
        "        self.test_summary = tf.summary.merge([self.summary_test_loss, self.summary_test_accuracy])\n",
        "\n",
        "    ##################################################################################\n",
        "    # Train\n",
        "    ##################################################################################\n",
        "\n",
        "    def train(self):\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # summary writer\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        if could_load:\n",
        "            epoch_lr = self.init_lr\n",
        "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
        "            counter = checkpoint_counter\n",
        "\n",
        "            if start_epoch >= int(self.epoch * 0.75) :\n",
        "                epoch_lr = epoch_lr * 0.01\n",
        "            elif start_epoch >= int(self.epoch * 0.5) and start_epoch < int(self.epoch * 0.75) :\n",
        "                epoch_lr = epoch_lr * 0.1\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            epoch_lr = self.init_lr\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        # loop for epoch\n",
        "        start_time = time.time()\n",
        "        for epoch in range(start_epoch, self.epoch):\n",
        "            if epoch == int(self.epoch * 0.5) or epoch == int(self.epoch * 0.75) :\n",
        "                epoch_lr = epoch_lr * 0.1\n",
        "            print(\"Epoch: [%2d]  time: %4.4f, learning_rate : %.4f\" \\\n",
        "                      % (epoch,   time.time() - start_time, epoch_lr))\n",
        "            # get batch data\n",
        "            for idx in range(start_batch_id, self.iteration):\n",
        "                batch_x = self.train_x[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "                batch_y = self.train_y[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "                batch_x = data_augmentation(batch_x, self.img_size, self.dataset_name)\n",
        "\n",
        "                train_feed_dict = {\n",
        "                    self.train_inptus : batch_x,\n",
        "                    self.train_labels : batch_y,\n",
        "                    self.lr : epoch_lr\n",
        "                }\n",
        "\n",
        "                test_feed_dict = {\n",
        "                    self.test_inptus : self.test_x,\n",
        "                    self.test_labels : self.test_y\n",
        "                }\n",
        "\n",
        "\n",
        "                # update network\n",
        "                _, summary_str, train_loss, train_accuracy = self.sess.run(\n",
        "                    [self.optim, self.train_summary, self.train_loss, self.train_accuracy], feed_dict=train_feed_dict)\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # test\n",
        "                summary_str, test_loss, test_accuracy = self.sess.run(\n",
        "                    [self.test_summary, self.test_loss, self.test_accuracy], feed_dict=test_feed_dict)\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # display training status\n",
        "                counter += 1\n",
        "            print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_accuracy: %.2f, test_accuracy: %.2f, learning_rate : %.4f\" \\\n",
        "                      % (epoch, idx, self.iteration, time.time() - start_time, train_accuracy, test_accuracy, epoch_lr))\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model\n",
        "            self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "        # save model for final step\n",
        "        self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return \"{}{}_{}_{}_{}\".format(self.model_name, self.res_n, self.dataset_name, self.batch_size, self.init_lr)\n",
        "\n",
        "    def save(self, checkpoint_dir, step):\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)\n",
        "\n",
        "    def load(self, checkpoint_dir):\n",
        "        print(\" [*] Reading checkpoints...\")\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "            counter = int(ckpt_name.split('-')[-1])\n",
        "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
        "            return True, counter\n",
        "        else:\n",
        "            print(\" [*] Failed to find a checkpoint\")\n",
        "            return False, 0\n",
        "\n",
        "    def test(self):\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "\n",
        "        if could_load:\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        test_feed_dict = {\n",
        "            self.test_inptus: self.test_x,\n",
        "            self.test_labels: self.test_y\n",
        "        }\n",
        "\n",
        "\n",
        "        test_accuracy = self.sess.run(self.test_accuracy, feed_dict=test_feed_dict)\n",
        "        print(\"test_accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6F_5rB85Jcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4db1aefe-ea77-4fb2-eca8-be219c4f7b42"
      },
      "source": [
        "!pip install utils"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZtgXAh68iWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08f8aadc-e703-49e0-ab6c-cab9dabaacc5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "' '   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw_FjUdPUeKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24c57fba-d3df-460f-ab47-854dfd3517d1"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "' '   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhaHRdX33hik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5988ac9d-6f81-4eb0-a63c-d88621c9a5bc"
      },
      "source": [
        "#from ResNet import ResNet\n",
        "import argparse\n",
        "from utils import *\n",
        "\n",
        "\"\"\"parsing and configuration\"\"\"\n",
        "def parse_args():\n",
        "    desc = \"Tensorflow implementation of ResNet\"\n",
        "    parser = argparse.ArgumentParser(description=desc)\n",
        "    parser.add_argument('--phase', type=str, default='train', help='train or test ?')\n",
        "    parser.add_argument('--dataset', type=str, default='tiny', help='[cifar10, cifar100, mnist, fashion-mnist, tiny')\n",
        "\n",
        "\n",
        "    parser.add_argument('--epoch', type=int, default=82, help='The number of epochs to run')\n",
        "    parser.add_argument('--batch_size', type=int, default=256, help='The size of batch per gpu')\n",
        "    parser.add_argument('--res_n', type=int, default=18, help='18, 34, 50, 101, 152')\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
        "\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n",
        "                        help='Directory name to save the checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs',\n",
        "                        help='Directory name to save training logs')\n",
        "\n",
        "    return check_args(parser.parse_args())\n",
        "\n",
        "\"\"\"checking arguments\"\"\"\n",
        "def check_args(args):\n",
        "    # --checkpoint_dir\n",
        "    check_folder(args.checkpoint_dir)\n",
        "\n",
        "    # --result_dir\n",
        "    check_folder(args.log_dir)\n",
        "\n",
        "    # --epoch\n",
        "    try:\n",
        "        assert args.epoch >= 1\n",
        "    except:\n",
        "        print('number of epochs must be larger than or equal to one')\n",
        "\n",
        "    # --batch_size\n",
        "    try:\n",
        "        assert args.batch_size >= 1\n",
        "    except:\n",
        "        print('batch size must be larger than or equal to one')\n",
        "    return args\n",
        "\n",
        "\n",
        "\"\"\"main\"\"\"\n",
        "def main():\n",
        "    # parse arguments\n",
        "    #args = parse_args()\n",
        "    # if args is None:\n",
        "    #   exit()\n",
        "\n",
        "    # open session\n",
        "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        args=()\n",
        "        cnn = ResNet(sess, args)\n",
        "\n",
        "        # build graph\n",
        "        cnn.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        show_all_variables()\n",
        "\n",
        "        #if args.phase == 'train' :\n",
        "        # launch the graph in a session\n",
        "        cnn.train()\n",
        "\n",
        "        print(\" [*] Training finished! \\n\")\n",
        "\n",
        "        cnn.test()\n",
        "        print(\" [*] Test finished!\")\n",
        "\n",
        "        # if args.phase == 'test' :\n",
        "        #     cnn.test()\n",
        "        #     print(\" [*] Test finished!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120.70756512369792\n",
            "64.1500758911213\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "WARNING:tensorflow:From <ipython-input-1-c155637b2286>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-c155637b2286>:108: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-c155637b2286>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "network/conv/conv2d/kernel:0 (float32_ref 3x3x3x64) [1728, bytes: 6912]\n",
            "network/conv/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_0/conv_0/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_0/conv_1/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_1/conv_0/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_1/conv_1/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
            "network/resblock1_0/conv_0/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x64x128) [8192, bytes: 32768]\n",
            "network/resblock1_0/conv_init/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/batch_norm_1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/batch_norm_1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_0/conv_1/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_0/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_0/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_1/conv_0/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_1/conv_1/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/batch_norm_0/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/batch_norm_0/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x128x256) [294912, bytes: 1179648]\n",
            "network/resblock2_0/conv_0/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x128x256) [32768, bytes: 131072]\n",
            "network/resblock2_0/conv_init/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/batch_norm_1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/batch_norm_1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_0/conv_1/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_0/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_0/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_1/conv_0/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_1/conv_1/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/batch_norm_0/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/batch_norm_0/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x256x512) [1179648, bytes: 4718592]\n",
            "network/resblock_3_0/conv_0/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x256x512) [131072, bytes: 524288]\n",
            "network/resblock_3_0/conv_init/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/batch_norm_1/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/batch_norm_1/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_0/conv_1/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_0/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_0/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_1/conv_0/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_1/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_1/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_1/conv_1/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/batch_norm/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/batch_norm/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/logit/dense/kernel:0 (float32_ref 512x10) [5120, bytes: 20480]\n",
            "network/logit/dense/bias:0 (float32_ref 10) [10, bytes: 40]\n",
            "Total size of variables: 11176970\n",
            "Total bytes of variables: 44707880\n",
            " [*] Reading checkpoints...\n",
            " [*] Failed to find a checkpoint\n",
            " [!] Load failed...\n",
            "Epoch: [ 0]  time: 0.0000, learning_rate : 0.0100\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}